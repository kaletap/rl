{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Lecture 4: Model-free prediction\n",
    "\n",
    "Prediction = Policy evaluation\n",
    "\n",
    "Notes from a Reinformcement Course by David Silver (https://www.youtube.com/watch?v=PnHCvfgC_ZA&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&index=4)\n",
    "\n",
    "We are getting rid of the assumption, that we know the Markov Descision Process. In particular, we don't know what the reward is in each state and what are the probabilities what state we are going to be at after taking an action and even what the states are exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy21\n",
    "Assignment for a Reinforcement Learning Course, mentioned at 16:15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implementation of Easy21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_card():\n",
    "    u = random.random()\n",
    "    color = 'black' if u < (2/3) else 'red'\n",
    "    new_card = random.randint(1, 10)\n",
    "    return new_card if 'black' else -new_card\n",
    "\n",
    "class State:\n",
    "    def __init__(self, dealer_value=None, player_value=None):\n",
    "        self.dealer = dealer_value or random.randint(1, 10)\n",
    "        self.player = player_value or 0\n",
    "        \n",
    "    def hit(self):\n",
    "        new_card = draw_card()\n",
    "        return State(self.dealer, self.player + new_card)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'State(dealer={self.dealer}, player={self.player})'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    \"\"\"Function simulating Easy21 evironment. Returns (state: State, reward: int)\"\"\"\n",
    "    if action == 'stick':  # let the dealer play\n",
    "        while state.dealer < 17:\n",
    "            state.dealer += draw_card()\n",
    "        if state.dealer > 21:\n",
    "            return state, 1\n",
    "        elif state.dealer > state.player:\n",
    "            return state, -1\n",
    "        else:\n",
    "            return state, 0\n",
    "    else:  # action == 'hit'\n",
    "        state = state.hit()\n",
    "        if state.player > 21:\n",
    "            return state, -1\n",
    "        else:\n",
    "            return state, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Monte-Carlo Control in Easy21"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
