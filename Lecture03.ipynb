{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Lecture 3\n",
    "## Planning by Dynamic Programming\n",
    "https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&index=3\n",
    "(David Silver)\n",
    "\n",
    "Dynamic Programming is a general approach to create algorithms if the problem has a structure in which we can divide it into smaller but similar problems. For example, shortest path could be viewed this way, because in order to find a shortest path from A to B in a graph, it is enough to find a shortest path to from each neighbor of A (or to each neighbor of B) and choose te best one. In this case, it might not be the most intuitive approach to solving this problem, but many problems can be solved using this principle (for example sequence alignment in bioinformatics (Needlemanâ€“Wunsch) algorithm, edit ( Levenshtein) distance, longest common subsequence in two strings, Viterbi algorithm and - most importantly for us - solving Bellman equations). More algorithms can be found here: https://en.wikipedia.org/wiki/Dynamic_programming. Even though these problems sound not related at all, seem complicated and have weird names and many authors, they all share the same underlying principle. And this principle was invented by Richard Bellman in 1950s.\n",
    "\n",
    "Now, about the problem we are trying to solve. We are considering planning and control, given that we have full knowledge of the world (modelled as Markov Decision Process). \n",
    "\n",
    "#### Planning\n",
    "Evaluating policy $\\pi$ in the MDP world.\n",
    "\n",
    "Input: MDP: $(\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma)$, policy $\\pi$\n",
    "\n",
    "Output: value funciton $v_{\\pi}$\n",
    "\n",
    "\n",
    "#### Control\n",
    "Actually finding the best policy.\n",
    "\n",
    "Input: MDP: $(\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma)$\n",
    "\n",
    "Output: *optimal* value funciton $v_{\\pi}$\n",
    "\n",
    "Note, that when we have optimal value function, we immediately have the optimal policy: do the action that maximises the value function.\n",
    "\n",
    "## Example\n",
    "### Defining MDP Process\n",
    "Consider a 4x4 grid world. If we are at state 0 or 15 we get a reward of 0 and stay there forever. Otherwise, the reward is -1. In each state we can go up, down, left or right. If it would take us outside of a grid, we stay in the same place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "grid_world = np.array(np.arange(16)).reshape(4, 4)\n",
    "grid_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: inheriting from int is completely useless \n",
    "# here, just for fun\n",
    "class State(int):\n",
    "    def __new__(cls, row, col):\n",
    "        return int.__new__(cls, row * 4 + col)\n",
    "    \n",
    "    def reward(self):  # in general reward might depend on action (but not in this example)\n",
    "        if self == 0 or self == 15:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    @property\n",
    "    def position(self):\n",
    "        return divmod(self, 4)\n",
    "\n",
    "class Action(ABC):\n",
    "    @abstractmethod\n",
    "    def direction(self):\n",
    "        pass\n",
    "\n",
    "class N(Action):\n",
    "    def direction(self):\n",
    "        return -1, 0\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"N\"\n",
    "    pass\n",
    "\n",
    "class S(Action):\n",
    "    def direction(self):\n",
    "        return 1, 0\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"S\"\n",
    "    pass\n",
    "\n",
    "class E(Action):\n",
    "    def direction(self):\n",
    "        return 0, 1\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"E\"\n",
    "    pass\n",
    "\n",
    "class W(Action):\n",
    "    def direction(self):\n",
    "        return 0, -1\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"W\"\n",
    "    pass\n",
    "\n",
    "actions = [N(), S(), W(), E()]\n",
    "\n",
    "grid_world = np.array([[State(*divmod(s, 4)) for s in row] for row in grid_world])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_position(row, col):\n",
    "    return 0 <= row < 4 and 0 <= col < 4\n",
    "\n",
    "def take_action(current_state: State, action: Action) -> State:\n",
    "    row, col = current_state.position\n",
    "    if (row, col) == (0, 0) or (row, col) == (3, 3):\n",
    "        return current_state\n",
    "    dirs = action.direction()\n",
    "    new_position = row + dirs[0], col + dirs[1]\n",
    "    if is_valid_position(*new_position):\n",
    "        return State(row + dirs[0], col + dirs[1])\n",
    "    else:\n",
    "        return current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic policy\n",
    "In each step we pick action randomly with equal probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_vector = np.ones(4) / 4\n",
    "\n",
    "def policy(state: State) -> Action:\n",
    "    return np.random.choice(actions, p=policy_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function\n",
    "We initalize the value function with all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing value function\n",
    "values = np.zeros_like(grid_world, dtype=np.float32)\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman equation for value function\n",
    "In general, if state after action was stochastic, we would have to average over all possible outcomes of an action.\n",
    "$$ v^{k+1}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) (R^a_s + \\gamma \\sum_{s' \\in \\mathcal{S}} P^a_{ss'} v_k(s')) $$\n",
    "\n",
    "In the case when the state after taking action is deterministic however, we obtain have only one element in the inner sum. It all comes down to averagin over neighboring states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1  # no discount factor\n",
    "old_values = values.copy()\n",
    "values = np.zeros_like(values)\n",
    "# Iterate over states\n",
    "for row in range(0, 4):\n",
    "    for col in range(0, 4):\n",
    "        for action, prob in zip(actions, policy_vector):\n",
    "            s = State(row, col)\n",
    "            new_state = take_action(s, action)\n",
    "            new_row, new_col = new_state.position\n",
    "            old_value = old_values[new_row][new_col]\n",
    "            values[row, col] += prob * (s.reward() + gamma * old_value)  # in general, we would have to average over possible outcomes of our action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1.],\n",
       "       [-1., -1., -1.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we performed one iteration of value update according to Bellman equation. We see, that the value of the first and last state stayed $0$, but the rest turned to $1$, that is immediate reward of being in this place. Let's perform more iterations and see how the value of this random policy changes when we do more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values(values):\n",
    "    old_values = values.copy()\n",
    "    values = np.zeros_like(values)\n",
    "    # Iterate over states\n",
    "    for row in range(0, 4):\n",
    "        for col in range(0, 4):\n",
    "            for action, prob in zip(actions, policy_vector):\n",
    "                s = State(row, col)\n",
    "                new_state = take_action(s, action)\n",
    "                new_row, new_col = new_state.position\n",
    "                old_value = old_values[new_row][new_col]\n",
    "                values[row, col] += prob * (s.reward() + gamma * old_value)  # in general, we would have to average over possible outcomes of our action\n",
    "    err = np.max((values - old_values) **2)**0.5  # we control maximum value of element-wise error\n",
    "    return values, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value function in time until convergence\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a1f24d57214aa68b1c1de44925efc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "1\n",
      "[[ 0.  -1.8 -2.  -2. ]\n",
      " [-1.8 -2.  -2.  -2. ]\n",
      " [-2.  -2.  -2.  -1.8]\n",
      " [-2.  -2.  -1.8  0. ]]\n",
      "\n",
      "2\n",
      "[[ 0.  -2.4 -2.9 -3. ]\n",
      " [-2.4 -2.9 -3.  -2.9]\n",
      " [-2.9 -3.  -2.9 -2.4]\n",
      " [-3.  -2.9 -2.4  0. ]]\n",
      "\n",
      "10\n",
      "[[ 0.  -6.6 -9.  -9.7]\n",
      " [-6.6 -8.3 -9.  -9. ]\n",
      " [-9.  -9.  -8.3 -6.6]\n",
      " [-9.7 -9.  -6.6  0. ]]\n",
      "\n",
      "20\n",
      "[[  0.   -9.7 -13.6 -14.9]\n",
      " [ -9.7 -12.4 -13.7 -13.6]\n",
      " [-13.6 -13.7 -12.4  -9.7]\n",
      " [-14.9 -13.6  -9.7   0. ]]\n",
      "\n",
      "30\n",
      "[[  0.  -11.5 -16.3 -17.9]\n",
      " [-11.5 -14.7 -16.3 -16.3]\n",
      " [-16.3 -16.3 -14.7 -11.5]\n",
      " [-17.9 -16.3 -11.5   0. ]]\n",
      "\n",
      "40\n",
      "[[  0.  -12.6 -17.9 -19.6]\n",
      " [-12.6 -16.1 -17.9 -17.9]\n",
      " [-17.9 -17.9 -16.1 -12.6]\n",
      " [-19.6 -17.9 -12.6   0. ]]\n",
      "\n",
      "50\n",
      "[[  0.  -13.2 -18.8 -20.6]\n",
      " [-13.2 -16.9 -18.8 -18.8]\n",
      " [-18.8 -18.8 -16.9 -13.2]\n",
      " [-20.6 -18.8 -13.2   0. ]]\n",
      "\n",
      "60\n",
      "[[  0.  -13.5 -19.3 -21.2]\n",
      " [-13.5 -17.4 -19.3 -19.3]\n",
      " [-19.3 -19.3 -17.4 -13.5]\n",
      " [-21.2 -19.3 -13.5   0. ]]\n",
      "\n",
      "70\n",
      "[[  0.  -13.7 -19.6 -21.5]\n",
      " [-13.7 -17.6 -19.6 -19.6]\n",
      " [-19.6 -19.6 -17.6 -13.7]\n",
      " [-21.5 -19.6 -13.7   0. ]]\n",
      "\n",
      "80\n",
      "[[  0.  -13.8 -19.8 -21.7]\n",
      " [-13.8 -17.8 -19.8 -19.8]\n",
      " [-19.8 -19.8 -17.8 -13.8]\n",
      " [-21.7 -19.8 -13.8   0. ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal value function in time until convergence\")\n",
    "values = np.zeros_like(grid_world, dtype=np.float32)\n",
    "n_iter = 100\n",
    "for i in tqdm(range(n_iter)):\n",
    "    values, err = update_values(values)\n",
    "    if i < 3 or i % 10 == 0:\n",
    "        print(i)\n",
    "        print(np.round(values, 1))\n",
    "        print()\n",
    "    if err < 0.01:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.      , -13.895283, -19.84483 , -21.826353],\n",
       "       [-13.895283, -17.863304, -19.845867, -19.84483 ],\n",
       "       [-19.84483 , -19.845867, -17.863304, -13.895284],\n",
       "       [-21.826355, -19.84483 , -13.895285,   0.      ]], dtype=float32)"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on new value function we can improve our policy from random to deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_policy(state_number: int) -> Action:\n",
    "    # q_function is the value v given the action a\n",
    "    q_function = []\n",
    "    for action in actions:\n",
    "        state = State(*divmod(state_number, 4))\n",
    "        new_state = take_action(state, action)\n",
    "        row, col = new_state.position\n",
    "        v = values[row, col]\n",
    "        q_function.append((action, v))\n",
    "    a, v = max(q_function, key=lambda t: t[1])  # action with the highest value\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[N, W, W, S],\n",
       "       [N, N, S, S],\n",
       "       [N, N, E, S],\n",
       "       [N, E, E, N]], dtype=object)"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_policy = np.vectorize(lambda s: improved_policy(s))(grid_world)\n",
    "new_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the policy that intuitively makes sense, it allows to quickly reach the top-left or bottom-right corners, where the value is the best. Note, that we would obtian this policy even before the convergence of value function (after only three iterations we would be able to obtain this policy).\n",
    "\n",
    "## General case\n",
    "In general, we wouldn't be able to obtain optimal policy after computing value function of random policy. However, we would be able to improve that policy, and use it to improve value function once again. This gives us algorithm for finding the best policy.\n",
    "\n",
    "Here is algorithm and a visualization from David Silver's slide:\n",
    "\n",
    "![](img/policy_iteration.png)\n",
    "\n",
    "(https://www.davidsilver.uk/teaching/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
